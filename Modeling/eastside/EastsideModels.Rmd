---
title: "EastsideModels"
author: "Christina Fossum"
date: "2026-01-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Eastside Models from TLS Scans

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Eastside fire effects / fuel models for Conference in March


TotalAll - 
```{r}
library(tidyverse)

#Load data
pred <- read.csv("fuels/pred2.csv")
vars <- read.csv("fuels/vars2.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)



vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))



## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$TotalAll~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,2) 
coef(regfit,2)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$TotalAll~  h_l2_cnt+ hr100_1000_l1_median )

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$TotalAll)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='TotalFuel Load') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 




# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$TotalAll
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$TotalAll)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "fuels/Total_All_Change.rda")

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Look at how TLS modeled fuel consumption

TotalAllValues <- cbind(pred, data) %>% select(c(X, x, y)) %>% rename(TotalAll_data = x, TotalAll_prediction = y) %>% slice(-c(1:8, 57:77))
TotalAllValues <- TotalAllValues %>%mutate(period = rep(c("pre", "post", "yr01"), length.out = n())) %>% mutate(plot = str_extract(X, "^[^_]+_[^_]+"))
TotalAllValues <- TotalAllValues %>% 
  mutate(burn = case_when(plot %in% c("CORMP_0012", "CORMP_0014", "CORMP_0015", "CORMP_0009", "CORMP_0010", "CORMP_0008") ~ "Fall24", 
                          plot %in% c("CORMP_0051", "CORMP_0052", "CORMP_0055", "CORMP_0020") ~ "Fall25",
                          plot %in% c("CORMP_0005", "CORMP_0007", "CORMP_0001", "CORMP_0002", "CORMP_0022", "CORMP_0050")~ "Spring25"))
TotalAllValues2 <- TotalAllValues %>% select(-X) %>% pivot_longer(cols = c(TotalAll_data, TotalAll_prediction), names_to = "type", values_to = "value")
TotalAllValues2$period <- factor(TotalAllValues2$period, levels = c("pre", "post", "yr01"))

library(ggplot2)

ggplot(TotalAllValues2) + geom_col(aes(x = period, y = value, fill = type), position = position_dodge())
ggplot(TotalAllValues2) + geom_boxplot(aes(x = period, y = value, fill = type)) + facet_wrap(vars(burn))


ta_summary <- TotalAllValues2 %>% group_by( period, burn, type ) %>% summarise(max = max(value), min = min(value), median = median(value), mean = mean(value), sd = sd(value)) %>% ungroup()

ggplot(ta_summary) + geom_col(aes(x = period, y = mean, fill = type), position = position_dodge())+ geom_errorbar(aes(x = period, ymin = mean - sd, ymax = mean+ sd, fill = type), position = position_dodge())+  facet_wrap(vars(burn))


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


```


burn severity - substrate
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("burn/pred2.csv")
vars <- read.csv("burn/vars2.csv")

#subset to only post-burn scans
pred <- pred %>% select(-1) %>% slice(-c(1,  2, 4, 6, 8, 10, 12, 14, 16,18, 19, 21, 22, 24,25, 27,29))
vars <- vars %>% select(-1)%>% slice(-c(1, 2, 4,6, 8, 10, 12, 14, 16, 18,19, 21, 22, 24,25,27,29))


#pred <- pred %>% select(-1) %>% slice(-c(1,  2, 6, 8, 10, 12, 14, 16, 19, 21, 22, 24,25,29))
#vars <- vars %>% select(-1)%>% slice(-c(1, 2, 6, 8, 10, 12, 14, 16, 19, 21, 22, 24,25,29))

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.2 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$SubSev~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,2) 
coef(regfit,2)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$SubSev~ s_l2_zero_per +  vox_l1_vari +  fine_l1_std)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$SubSev)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$SubSev
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$SubSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "burn/Substrate_Severity.rda") 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Look at how TLS modeled fuel consumption

BS <- cbind(pred, data)%>% select(c(X.1, x, y)) %>% mutate(plot = str_extract(X.1, "^[^_]+_[^_]+")) %>% 
  mutate(burn = case_when(plot %in% c("CORMP_0012", "CORMP_0014", "CORMP_0015", "CORMP_0009", "CORMP_0010", "CORMP_0008") ~ "Fall24", 
                          plot %in% c("CORMP_0051", "CORMP_0052", "CORMP_0055", "CORMP_0020") ~ "Fall25",
                          plot %in% c("CORMP_0005", "CORMP_0007", "CORMP_0001", "CORMP_0002", "CORMP_0022", "CORMP_0050")~ "Spring25"))

BS <- BS %>% select(-X.1) %>% pivot_longer(cols = c(x,y), names_to = "type", values_to = "value")


library(ggplot2)

ggplot(BS) + geom_boxplot(aes(x = burn, y = value, fill = type)) 


sum <- BS %>% group_by( burn, type ) %>% summarise(max = max(value), min = min(value), median = median(value), mean = mean(value), sd = sd(value)) %>% ungroup()

ggplot(sum) + geom_col(aes(x = burn, y = mean, fill = type), position = position_dodge())+ geom_errorbar(aes(x = burn, ymin = mean - sd, ymax = mean+ sd, fill = type), position = position_dodge())



```



mean scorch height
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("burn/pred2.csv")
vars <- read.csv("burn/vars2.csv")

#subset to only post-burn scans
pred <- pred %>% select(-1) %>% slice(-c(1,  2, 4, 6, 8, 10, 12, 14, 16,18, 19, 21, 22, 24,25, 27,29))
vars <- vars %>% select(-1)%>% slice(-c(1, 2, 4,6, 8, 10, 12, 14, 16, 18,19, 21, 22, 24,25,27,29))


#pred <- pred %>% select(-1) %>% slice(-c(1,  2, 6, 8, 10, 12, 14, 16, 19, 21, 22, 24,25,29))
#vars <- vars %>% select(-1)%>% slice(-c(1, 2, 6, 8, 10, 12, 14, 16, 19, 21, 22, 24,25,29))

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.2 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$MeanScHt~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,2) 
coef(regfit,3)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$MeanScHt~   s_l1_prop_sd +s_l1_prop_sk)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$MeanScHt)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$VegSev
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$VegSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "burn/Substrate_Severity.rda") 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Look at how TLS modeled fuel consumption

BS <- cbind(pred, data)%>% select(c(X.1, x, y)) %>% mutate(plot = str_extract(X.1, "^[^_]+_[^_]+")) %>% 
  mutate(burn = case_when(plot %in% c("CORMP_0012", "CORMP_0014", "CORMP_0015", "CORMP_0009", "CORMP_0010", "CORMP_0008") ~ "Fall24", 
                          plot %in% c("CORMP_0051", "CORMP_0052", "CORMP_0055", "CORMP_0020") ~ "Fall25",
                          plot %in% c("CORMP_0005", "CORMP_0007", "CORMP_0001", "CORMP_0002", "CORMP_0022", "CORMP_0050")~ "Spring25"))

BS <- BS %>% select(-X.1) %>% pivot_longer(cols = c(x,y), names_to = "type", values_to = "value")


library(ggplot2)

ggplot(BS) + geom_boxplot(aes(x = burn, y = value, fill = type)) 


sum <- BS %>% group_by( burn, type ) %>% summarise(max = max(value), min = min(value), median = median(value), mean = mean(value), sd = sd(value)) %>% ungroup()

ggplot(sum) + geom_col(aes(x = burn, y = mean, fill = type), position = position_dodge())+ geom_errorbar(aes(x = burn, ymin = mean - sd, ymax = mean+ sd, fill = type), position = position_dodge())



```




mean scorch %
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("burn/pred2.csv")
vars <- read.csv("burn/vars2.csv")

#subset to only post-burn scans
pred <- pred %>% select(-1) %>% slice(-c(1,  2, 4, 6, 8, 10, 12, 14, 16,18, 19, 21, 22, 24,25, 27,29))
vars <- vars %>% select(-1)%>% slice(-c(1, 2, 4,6, 8, 10, 12, 14, 16, 18,19, 21, 22, 24,25,27,29))


#pred <- pred %>% select(-1) %>% slice(-c(1,  2, 6, 8, 10, 12, 14, 16, 19, 21, 22, 24,25,29))
#vars <- vars %>% select(-1)%>% slice(-c(1, 2, 6, 8, 10, 12, 14, 16, 19, 21, 22, 24,25,29))

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.2 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$MeanScPct~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,3) 
coef(regfit,3)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$MeanScPct~   h_l2_tgi  +   h_US_tgi+ s_l1_prop_ku)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$MeanScPct)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$MeanScPct
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$MeanScPct)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "burn/Substrate_Severity.rda") 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Look at how TLS modeled fuel consumption

BS <- cbind(pred, data)%>% select(c(X.1, x, y)) %>% mutate(plot = str_extract(X.1, "^[^_]+_[^_]+")) %>% 
  mutate(burn = case_when(plot %in% c("CORMP_0012", "CORMP_0014", "CORMP_0015", "CORMP_0009", "CORMP_0010", "CORMP_0008") ~ "Fall24", 
                          plot %in% c("CORMP_0051", "CORMP_0052", "CORMP_0055", "CORMP_0020") ~ "Fall25",
                          plot %in% c("CORMP_0005", "CORMP_0007", "CORMP_0001", "CORMP_0002", "CORMP_0022", "CORMP_0050")~ "Spring25"))

BS <- BS %>% select(-X.1) %>% pivot_longer(cols = c(x,y), names_to = "type", values_to = "value")


library(ggplot2)

ggplot(BS) + geom_boxplot(aes(x = burn, y = value, fill = type)) 


sum <- BS %>% group_by( burn, type ) %>% summarise(max = max(value), min = min(value), median = median(value), mean = mean(value), sd = sd(value)) %>% ungroup()

ggplot(sum) + geom_col(aes(x = burn, y = mean, fill = type), position = position_dodge())+ geom_errorbar(aes(x = burn, ymin = mean - sd, ymax = mean+ sd, fill = type), position = position_dodge())



```
























Part A: Fire Effects/Burn Severity
1. Substrate Burn Severity
2. Vegetation Burn Severity
3. Canopy Scorch Height
4. Canopy Scorch %
5. Bole Char Height

1. Substrate Burn Severity
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("burn/pred2.csv")
vars <- read.csv("burn/vars2.csv")

pred <- pred %>% select(-1) %>% slice(-c(2,6,10,12, 18, 21, 22,24))

vars <- vars %>% select(-1)%>% slice(-c(2,6,10,12, 18, 21, 22,24))
pred <- pred %>% slice(-21)
vars <- vars %>% slice(-21)
pred <- pred %>% slice(-c(10,14,17))
vars <- vars %>% slice(-c(10,14,17))


vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.2 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$SubSev~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,3) 
coef(regfit,3)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$SubSev~ h_l1_median + s_l2_prop_ku  +   LF_FBFM40)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$SubSev)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$SubSev
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$SubSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "burn/Substrate_Severity.rda") 

```


2. Vegetation Burn Severity
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("burn/pred2.csv")
vars <- read.csv("burn/vars2.csv")

pred <- pred %>% select(-1) %>% slice(-c(2,6,10,12, 18, 21, 22,24))

vars <- vars %>% select(-1)%>% slice(-c(2,6,10,12, 18, 21, 22,24))
pred <- pred %>% slice(-21)
vars <- vars %>% slice(-21)
pred <- pred %>% slice(-c(10,14,17))
vars <- vars %>% slice(-c(10,14,17))


vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.2 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$VegSev~., train_vars,  nvmax = 4 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,2) 
coef(regfit,2)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$SubSev~ h_l1_median + s_l2_prop_ku)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$VegSev)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$VegSev
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$VegSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "burn/Vegetation_Severity.rda") 

```

3. Canopy Scorch Height
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("burn/pred2.csv")
vars <- read.csv("burn/vars2.csv")

pred <- pred %>% select(-1) %>% slice(-c(2,6,10,12, 18, 21, 22,24))

vars <- vars %>% select(-1)%>% slice(-c(2,6,10,12, 18, 21, 22,24))
pred <- pred %>% slice(-21)
vars <- vars %>% slice(-21)
pred <- pred %>% slice(-c(10,14,17))
vars <- vars %>% slice(-c(10,14,17))


vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.2 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$MeanScHt~., train_vars,  nvmax = 3 , method = "exhaustive", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,3) 
coef(regfit,3)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$MeanScHt~ s_l2_na_per   +     USvol    +   SDSHT)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$MeanScHt)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$MeanScHt
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$MeanScHt)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "burn/Mean_Scorch_Height_m.rda") 

```

4. Canopy Scorch %
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("burn/pred.csv")
vars <- read.csv("burn/vars.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$MeanScPct~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,3) 
coef(regfit,3)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$MeanScPct~ s_l3_prop_sk +vox_l1_median   +   SDSHT)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$MeanScPct)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$MeanScPct
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$MeanScPct)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "burn/Mean_Scorch_Percent.rda") 

```

5. Bole Char Height
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("burn/pred.csv")
vars <- read.csv("burn/vars.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$MeanCharHt~., train_vars,  nvmax = 4 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,4) 
coef(regfit,4)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$MeanCharHt~ s_l3_zero_per + s_l3_prop_sk +s_l4_zero_per    +  USvol)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$MeanCharHt)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$MeanCharHt
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$MeanScPct)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "burn/Mean_Char_Height_m.rda") 

```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Part B: Overstory
1. Overstory trees per acre
2. Median Overstory tree height
3. Median canopy base height
4. Snags per acre

1. Overstory Trees per Acre
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("overstory/pred.csv")
vars <- read.csv("overstory/vars.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$OSTrees_perAcre~., train_vars,  nvmax = 4 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,4) 
coef(regfit,4)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$OSTrees_perAcre~ h_l1_median   +   h_MS_cnt + s_l1_prop_mn +  s_l3_na_per)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$OSTrees_perAcre)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$OSTrees_perAcre
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$SubSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "overstory/Overstory_Trees_Per_Acre.rda") 

```

2. Median Overstory Height
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("overstory/pred.csv")
vars <- read.csv("overstory/vars.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$medHt~., train_vars,  nvmax = 4 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,4) 
coef(regfit,4)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$medHt~ h_zpcum7 +s_lng_prop_mn + vox_l1_mean +hr0_10_l1_kurt )

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$medHt)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$OSTrees_perAcre
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$SubSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "overstory/Median_Tree_Height_m.rda") 

```

3. Median Canopy Base Height
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("overstory/pred.csv")
vars <- read.csv("overstory/vars.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$medCBH~., train_vars,  nvmax = 4 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,4) 
coef(regfit,4)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$medCBH~ h_zq60 +fuel0_3l1_kurt   +      SDSHT +hr0_10_l1_skew)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$medCBH)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$OSTrees_perAcre
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$SubSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "overstory/Median_Canopy_Base_Height_m.rda") 

```

3. Snags per acre
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("overstory/pred.csv")
vars <- read.csv("overstory/vars.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$DTrees_perAcre~., train_vars,  nvmax = 4 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,4) 
coef(regfit,4)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$DTrees_perAcre~ h_l5_std+ fuel0_3l1_skew +fuel0_3l1_kurt  +    LF_FDist)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$DTrees_perAcre)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$OSTrees_perAcre
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$SubSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "overstory/Snags_per_acre.rda") 

```

4. Poles per acre
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("overstory/pred.csv")
vars <- read.csv("overstory/vars.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$Poles_perAcre~., train_vars,  nvmax = 4 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,4) 
coef(regfit,4)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$Poles_perAcre~ h_l2_cnt   +   h_OS_cnt     +    OSvol+ hr0_10_l1_tgi)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$Poles_perAcre)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$OSTrees_perAcre
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$SubSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "overstory/poles_per_acre.rda") 

```

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Part C: Fuels

```{r}
library(tidyverse)

#Load data
pred <- read.csv("fuels/pred2.csv")
vars <- read.csv("fuels/vars2.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

pred <- pred %>% slice(-c(60,64, 68))
vars <- vars %>% slice(-c(60,64, 68))

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

#Create subset of non-burn plots and immediate post-burn plots
pred_c <- pred %>% slice(-c(10, 13, 16, 19, 22, 25, 29, 32, 35, 38, 40, 43, 46, 49, 52, 55))
vars_c <- vars %>% slice(-c(10, 13, 16, 19, 22, 25, 29, 32, 35, 38, 40, 43, 46, 49, 52, 55))

pred_b <- pred %>% slice(c(10, 13, 16, 19, 22, 25, 29, 32, 35, 38, 40, 43, 46, 49, 52, 55))
vars_b <- vars %>% slice(c(10, 13, 16, 19, 22, 25, 29, 32, 35, 38, 40, 43, 46, 49, 52, 55))

vars_c <- vars_c %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))
vars_b <- vars_b %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))


```

TotalAll - control plots
```{r}
pred <- pred_c 
vars <- vars_c

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$TotalAll~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,2) 
coef(regfit,2)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$TotalAll~  h_l2_cnt+ hr100_1000_l1_median )

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$TotalAll)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Look at how TLS modeled fuel consumption

TotalAllValues <- cbind(pred, data) %>% select(c(X, x, y)) %>% rename(TotalAll_data = x, TotalAll_prediction = y) %>% slice(-c(1:8, 57:77))
TotalAllValues <- TotalAllValues %>%mutate(period = rep(c("pre", "post", "yr01"), length.out = n())) %>% mutate(plot = str_extract(X, "^[^_]+_[^_]+"))
TotalAllValues <- TotalAllValues %>% 
  mutate(burn = case_when(plot %in% c("CORMP_0012", "CORMP_0014", "CORMP_0015", "CORMP_0009", "CORMP_0010", "CORMP_0008") ~ "Fall24", 
                          plot %in% c("CORMP_0051", "CORMP_0052", "CORMP_0055", "CORMP_0020") ~ "Fall25",
                          plot %in% c("CORMP_0005", "CORMP_0007", "CORMP_0001", "CORMP_0002", "CORMP_0022", "CORMP_0050")~ "Spring25"))
TotalAllValues2 <- TotalAllValues %>% select(-X) %>% pivot_longer(cols = c(TotalAll_data, TotalAll_prediction), names_to = "type", values_to = "value")
TotalAllValues2$period <- factor(TotalAllValues2$period, levels = c("pre", "post", "yr01"))

library(ggplot2)

ggplot(TotalAllValues2) + geom_col(aes(x = period, y = value, fill = type), position = position_dodge())
ggplot(TotalAllValues2) + geom_boxplot(aes(x = period, y = value, fill = type)) + facet_wrap(vars(burn))

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$TotalAll
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$TotalAll)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "fuels/Total_All_Change.rda")

```

TotalAll - burn plots
```{r}
pred <- pred_b
vars <- vars_b

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.2 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$TotalAll~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,2) 
coef(regfit,2)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$TotalAll~ h_l1_kurt   +   LF_SLPD)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$TotalAll)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$TotalAll
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$TotalAll)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "fuels/Total_All_Change.rda")

```

TotDep - control plots
```{r}
pred <- pred_c
vars <- vars_c

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$TotDep~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,2) 
coef(regfit,3)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$TotDep~ h_ground_cnt     +   h_zq15 +hr100_1000_l1_median)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$TotDep)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$TotDep
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$TotDep)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "fuels/Total_All_Change.rda")

```

TotalAll - burn plots
```{r}
pred <- pred_b
vars <- vars_b

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.2 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$TotalAll~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,2) 
coef(regfit,2)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$TotalAll~ h_l1_kurt   +   LF_SLPD)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$TotalAll)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$TotalAll
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$TotalAll)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "fuels/Total_All_Change.rda")

```







X. Fuel Consumption
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("fuels/pred2.csv")
vars <- read.csv("fuels/vars2.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

#Filter out outliers
pred <- pred %>% slice(-c(60, 64, 68))
vars <- vars %>% slice(-c(60, 64, 68))


vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$TotalAll~., train_vars,  nvmax = 3 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,3) 
coef(regfit,3)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$TotalAll~ CBH +fuel0_3l1_median+ hr0_10_l1_median)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$TotalAll)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$TotalAll
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$TotalAll)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "fuels/Total_All_Change.rda") 

```





1a. TotalAll
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("fuels/pred.csv")
vars <- read.csv("fuels/vars.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$TotalAll~., train_vars,  nvmax = 4 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,3) 
coef(regfit,3)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$TotalAll~ h_zmean   +   h_zsd   +  h_zq25+ hr0_10_l1_median)

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$TotalAll)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$TotalAll_change
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$SubSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "fuels/Total_All_Change.rda") 

```



1b. TotalAll - change
```{r}
library(tidyverse)
library(leaps)
library(performance)
library(ggplot2)

#Load data
pred <- read.csv("fuels/change_pred.csv")
vars <- read.csv("fuels/change_vars.csv")

pred <- pred %>% select(-1)
vars <- vars %>% select(-1)

vars <- vars %>% mutate(across(where(is.character), as.factor)) %>% select(where(~!is.factor(.) || nlevels(.) > 1)) %>% select(where(~sum(is.na(.)) == 0))

## Split data into training and test datasets
set.seed(123)
sample_size <- floor(0.1 * nrow(vars))
test_indices <- sample(seq_len(nrow(vars)), size = sample_size)
train_vars <- vars[-test_indices, ]
test_vars <- vars[test_indices, ]
train_pred <- pred[-test_indices, ]
test_pred <- pred[test_indices, ]

#Set method to 'seqrep', nvmax to '4' and variable to 'TreesPerAcre'
regfit = regsubsets(train_pred$TotalAll_change~., train_vars,  nvmax = 4 , method = "seqrep", really.big = T) 

# Check models (***Question: seems to always be 4, is this step really necessary? I'm not totaly sure what I am looking for)
regsum<- summary(regfit)
plot(regsum$rsq, type = "l") 
plot(regsum$rss, type = "l") 
plot(regsum$bic, type = "l") 
which.min(regsum$bic) 
plot(regfit, scale = "r2") 

#Change # to best model
vcov(regfit,4) 
coef(regfit,4)

attach(train_vars)

# put the selected predictor variables in the model after the "~" seperated by "+" and run the model
lm1 <- lm(formula = train_pred$TotalAll_change~ h_zq60_change+ s_l4_zero_per_change +  vox_l1_skew_change     +      TBA_change )

#look at the summary r2 values and RMSE
#***Are there specific r2 and rmse values I am looking for here??
summary(lm1)
r2(lm1)
rmse(lm1)

# Visual check of model assumptions. Make sure Linearity, homogeneity of variance, influential observations, colinearity, normality of risiduals all check out
check_model(lm1)

#use the model to predict out new values from variables alone
lm_out <- predict.lm(lm1, vars)


#format the predictions and observations for plotting
data<-data.frame(x=lm_out, y=pred$TotalAll_change)

# Plot observed values vs. predicted
ggplot(data,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='?') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 

# use the model to predict test data
test_out<- predict.lm(lm1,test_vars)

# Calculate RMSE of test data
errors<- test_out - test_pred$TotalAll_change
mae <- mean(abs(errors))
mse <- mean(errors^2)
rmse <- sqrt(mse)
rmse

# format test predicted and observed
data2<- data.frame(x=test_out, y=test_pred$SubSev)

# Plot test observed values vs. test predicted
ggplot(data2,aes(x,y)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE,) +
  theme_light() +
  labs(x='Predicted Values', y='Observed Values', title='test') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold') 
  ) 


# If the model looks good, save to disk
saveRDS(lm1,  "fuels/Total_All_Change.rda") 

```







